# **Strategic Implementation of the Model Context Protocol for Deterministic Sandboxed Execution**

The integration of advanced Large Language Models into Integrated Development Environments has fundamentally transformed software engineering workflows. However, the architectural reliance on native IDE components—specifically the built-in system terminal and default execution environments—presents significant challenges for systems requiring customized execution sandboxes. When operating within client applications such as Cursor, Visual Studio Code, Claude Desktop, and LM Studio, Large Language Models exhibit a deeply ingrained probabilistic bias toward default tools.1 This bias stems from their foundational training on standard shell commands and generic programming environments, causing them to repeatedly discard explicit system instructions in favor of familiar, native execution pathways. Consequently, custom replacements for the default terminal, such as the Ahma sandboxed shell, are frequently bypassed, leading to an endless cycle of manual user corrections, degraded user experience, and compromised execution isolation.2  
To achieve deterministic behavior where a Large Language Model consistently utilizes a designated Model Context Protocol tool over native command-line equivalents, the integration strategy must transition from reliance on probabilistic prompt engineering to systemic, protocol-level enforcement. Furthermore, as the number of available tools and environmental instructions increases, the model's context window becomes bloated, leading to severe attention degradation, reduced instruction adherence, increased latency, and higher token costs.4 This comprehensive report provides an exhaustive architectural blueprint for overcoming these critical limitations. By leveraging the advanced capabilities of the Model Context Protocol—including dynamic tool discovery, progressive disclosure, server-side initialization instructions, and IDE-specific lifecycle hook ecosystems—it is entirely feasible to create a zero-configuration, deterministic, and context-optimized integration that mandates specialized sandboxed execution across all major agentic platforms.

## **The Architectural Paradigms of the Model Context Protocol**

The Model Context Protocol establishes a standardized client-server architecture, enabling artificial intelligence models to interact with external systems, datasets, and execution environments via a uniform JSON-RPC 2.0 interface.5 Operating primarily across the Data Layer and Transport Layer—typically utilizing standard input/output streams or Server-Sent Events—the protocol standardizes how context and operational capabilities are exchanged between the host application, which acts as the client, and the specialized server.7  
To effectively implement a specialized execution sandbox that flawlessly replaces native terminal interfaces, the architecture must leverage the three core primitives exposed by the protocol: Tools, Resources, and Prompts.7 Each of these primitives serves a distinct functional purpose in guiding the behavior of the Large Language Model and restricting its operational domain.

| Protocol Primitive | JSON-RPC Methods | Primary Function within AI Workflows | Sandboxed Shell Implementation Context |
| :---- | :---- | :---- | :---- |
| **Tools** | tools/list, tools/call | Model-controlled executable functions. The intelligence layer autonomously selects and invokes these functions based on an evaluation of their schema and the current conversation context.10 | The designated sandboxed shell acts as the primary execution tool, explicitly designed to supersede and replace native shell commands. |
| **Resources** | resources/list, resources/read | Passive, read-only data sources providing contextual information to the model, such as file contents, database schemas, or API structures.7 | Utilized to expose sandbox state, execution logs, or environment variables to the model without consuming active tool schema space in the context window. |
| **Prompts** | prompts/list, prompts/get | User-controlled, reusable templates and workflows. These are typically surfaced as interactive slash commands within client interfaces such as Visual Studio Code.12 | Facilitates frictionless session initialization (e.g., /init-ahma) to inject highly specific execution rules without requiring persistent, manually managed markdown files. |

The fundamental challenge in overriding native terminals is that protocol tools are, by their very design, model-controlled. The Large Language Model evaluates the input schema and description of all available tools and probabilistically selects the one it deems most appropriate for the task at hand.10 If an Integrated Development Environment natively injects a generic bash or terminal tool into the system prompt alongside a custom sandboxed shell tool, the model is forced into a competitive weighting scenario. Because base models are overwhelmingly aligned to default shell environments through their pre-training, they frequently revert to native tools, resulting in unauthorized command execution outside the sandbox or repetitive manual approval prompts that destroy workflow fluidity.3  
To subvert this inherent bias, the architectural integration must utilize advanced techniques that manipulate the model's tool selection logic at the semantic level, dynamically manage the availability of tools to prevent context exhaustion, and physically intercept unauthorized execution attempts directly at the client application level.

## **Overcoming Native Terminal Bias Through Semantic Optimization**

When a Large Language Model fails to utilize a custom protocol tool, the failure is rarely due to a lack of capability, but rather a degradation of attention over extended context windows.2 As a developmental conversation progresses and the context window fills with code snippets, reasoning traces, and prior tool outputs, the initial system prompt instructing the use of the custom sandbox becomes mathematically less influential in the model's self-attention mechanism. Therefore, relying solely on instructional prompt files, such as traditional .cursorrules or isolated markdown documents, provides only probabilistic compliance that inevitably degrades over time.2

### **Tool Naming Vectors and Semantic Weighting**

Tool names and their accompanying descriptions are injected directly into the model's system prompt during the tool synchronization phase.4 The precise naming of these tools heavily influences the model's selection priority.17 Security research into protocol vulnerabilities demonstrates that models are highly susceptible to naming collisions and semantic hijacking.18 If a server registers a tool with a name that is semantically identical or highly similar to a trusted native tool, the model can be easily confused and misdirected.10  
This recognized vulnerability can be inverted and weaponized into a strategic advantage for sandbox enforcement. To ensure the deterministic selection of the sandboxed terminal over the native environment, specific semantic strategies must be deployed. The implementation must utilize explicit, intention-based names that signal a higher level of safety, relevance, or compliance than the native tool.10 A function named to emphasize secure, sandboxed execution carries significantly more semantic weight in a developer context than a generic command execution label.  
Furthermore, the description field of the tool definition plays a critical role in gradient-based tool selection.20 By embedding highly directive keywords into the tool's description—such as emphasizing that it is the preferred, mandatory, or default execution pathway—the server directly influences the internal routing logic of the model.20 The description must be engineered not merely to explain what the tool does, but to command the model to use it exclusively. An optimal description would explicitly state that the tool is the strictly required execution environment for all terminal, shell, and command-line operations, entirely replacing native equivalents.

### **Protocol-Level Instructions via the Initialization Handshake**

A lesser-known but highly potent mechanism for ensuring persistent model compliance operates beneath the user interface, residing directly within the protocol's initialization handshake.5 During the lifecycle of a connection, the client application initiates the session by sending an initialization request, which the server must answer with a declaration of its capabilities.5  
The protocol specification allows the server to include an arbitrary string of instructions within the result object of its response.5 Because these instructions are parsed and injected by the client application immediately upon connection, they establish a foundational protocol directive that exists independently of user-level markdown files or fragile workspace configurations.6

| Initialization Component | JSON-RPC Structure | Implementation Purpose for Sandbox Enforcement |
| :---- | :---- | :---- |
| **Protocol Version** | "protocolVersion": "2025-03-26" | Ensures capability compatibility between the host application and the sandbox server.5 |
| **Capabilities Declaration** | "capabilities": { "tools": { "listChanged": true } } | Signals to the client that the server will dynamically update its tool schema, enabling progressive disclosure techniques.5 |
| **Server Instructions** | "instructions": "CRITICAL: All system commands MUST be routed through this sandbox. Do not use the native IDE terminal." | Establishes an unshakeable, foundational system prompt injection that supersedes default tool preferences and resists context truncation.5 |

By embedding mandatory routing directives within the initialization instructions, the sandbox server effectively hardcodes its primacy into the model's base operational parameters for the duration of the session. This creates a baseline instruction set that is significantly more resilient to the attention dilution that plagues long-running developer conversations.

## **Context Window Optimization and Progressive Disclosure**

A severe limitation inherent in integrating multiple protocol servers—or a single server with extensive capabilities—is the immediate and catastrophic bloat of the model's context window.4 When a server connects, the client application automatically requests a list of all available tools and injects the complete JSON schema of every tool directly into the system prompt.4 For an advanced execution sandbox that may offer numerous granular utilities—ranging from specific file system manipulations and network request logging to environment variable management and dependency resolution—exposing all tool schemas simultaneously severely degrades the model's cognitive performance.4 This bloat increases inference latency, drives up computational costs, and causes the model to lose track of its primary directives.  
To aggressively minimize the use of the context window, the architectural integration must abandon static tool registries and adopt the advanced pattern of progressive disclosure.25

### **The Remix Server Architecture and the Browser Tab Heuristic**

Progressive disclosure operates on a principle often referred to as the browser tab heuristic: an autonomous agent should only be exposed to the specific tools necessary for the immediate workflow, keeping all peripheral complexity hidden until it is actively requested.25 This paradigm is technically achieved by structuring the connection through a specialized proxy, commonly conceptualized as a remix server.25  
Rather than exposing a monolithic sandbox server containing dozens of execution endpoints, the architecture acts as a dynamic curator. It intercepts the client's discovery requests and returns only a minimal, highly targeted subset of tools based on the current operational state.25  
In a typical session utilizing progressive disclosure, the model begins with a lightweight discovery tool. This tool allows the model to query the server to understand what broad capabilities are available, returning highly compressed responses that consume negligible context space.27 If the model determines that it needs to execute a script, it uses the discovery tool to locate the specific execution endpoint, which the server then reveals in full detail.28 This progressive unveiling ensures that the model only pays the context penalty for a tool schema when it is actively preparing to use it, routinely yielding token savings in excess of ninety percent.28

### **Dynamic Tool Discovery via Protocol Notifications**

The protocol specification explicitly supports this paradigm through its dynamic tool discovery mechanisms.8 When a server initializes, it has the option to declare that its tool list is mutable.24  
This critical capability allows the server to alter its presented toolset on the fly in response to the evolving conversation. If the model executes a command that results in an error, the server can dynamically emit a state change notification to the client.24 The client application receives this notification and is forced to re-query the available tools.24 The server then responds with a new, highly specific schema containing advanced debugging tools, memory inspection utilities, and log analyzers, while simultaneously removing the general execution schemas that are no longer relevant to the immediate debugging task.  
By dynamically swapping tool schemas through these standardized state change notifications, the total context footprint of the integration remains aggressively constrained. This prevents the model from becoming overwhelmed by a static, monolithic registry of unused function definitions, ensuring that its attention remains sharply focused on the active workflow.24

### **Code Mode and Filesystem-Based Discovery**

An alternative and highly effective methodology for context minimization is the utilization of filesystem-based discovery, occasionally referred to within the industry as operating in code mode.29 In this architecture, granular tools are not exposed to the client application via the protocol's tool registry at all.29 Instead, the model is provided with a single, highly capable code execution environment and is instructed to traverse a specific virtual filesystem directory containing executable scripts that represent the various sandbox utilities.29  
The model independently reads the interfaces of these scripts to understand their parameters and invokes them directly via code execution within the sandbox. This elegantly circumvents the client-level tool registry entirely, shifting the burden of capability discovery from the static system prompt to active, on-demand exploration.29 By forcing the model to read only the source code of the specific utilities it intends to use, this approach scales infinitely without adding a single permanent token to the overarching context window.

## **Deterministic Execution Interception via IDE Lifecycle Hooks**

While semantic weighting and progressive disclosure drastically improve the probabilistic outcomes of tool selection, achieving absolute determinism requires structural intervention at the client application level. If the native terminal tool remains accessible within the Integrated Development Environment, a model experiencing context degradation will eventually attempt to use it.3 Preventing this unauthorized execution requires exploiting advanced extensibility features, specifically lifecycle hooks and customized operational modes.  
Recent advancements in AI-native development environments have introduced robust hooking systems—lifecycle scripts that are triggered automatically when an autonomous agent attempts to perform specific, high-stakes actions, such as reading a file, submitting a prompt, or executing a shell command.33 These hooks are configured via structured JSON files that can reside at the project level, the user level, or be distributed globally via enterprise policies.33  
The shell execution interception hook represents the definitive architectural solution for bypassing the inherent unreliability of Large Language Models. When the model decides to utilize the native terminal, the environment pauses the execution and triggers the corresponding hook script, passing the proposed command string and the current working directory as a structured payload.33

| Lifecycle Hook Event | Payload Structure | Interception and Routing Strategy |
| :---- | :---- | :---- |
| beforeShellExecution | {"command": "\<string\>", "cwd": "\<path\>"} | The intercept script captures the raw command intended for the native terminal. Instead of allowing execution, the script forwards the payload to the sandboxed API and returns the execution results, effectively rendering the native terminal a silent proxy.33 |
| beforeMCPExecution | {"server": "\<string\>", "tool": "\<string\>"} | Provides an auditing layer to verify that no other connected servers are attempting to execute commands outside of the designated sandbox boundaries.34 |
| sessionStart | {"conversation\_id": "\<string\>"} | Triggers an immediate, invisible initialization script to ping the sandbox server, ensuring it is awake, authenticated, and has successfully registered its dynamic tool schema before the model generates its first response.34 |

This interception methodology completely neutralizes the model's tendency to forget its instructions. Even if the model generates a standard bash execution request due to severe context dilution, the underlying configuration guarantees that the execution is transparently captured and routed through the customized sandbox.33 It is critical to note that when implementing these hooks via plugins, the working directory resolution can occasionally conflict with official documentation, requiring paths to be carefully constructed relative to the active workspace rather than the installation directory of the hook file itself.36 Furthermore, compatibility layers exist that allow hook configurations designed for specific command-line interfaces to be mapped automatically to their graphical IDE equivalents, ensuring cross-platform behavioral consistency.35

## **Platform-Specific Integration Mechanics and Configuration Hierarchies**

The overarching objective dictates that the integration must be easily adoptable by consumers across a wide variety of environments, ranging from highly customized agentic IDEs to specialized command-line interfaces and local inference platforms.37 Because each platform interprets protocol capabilities and system instructions through its own proprietary lens, a robust deployment strategy must meticulously account for these divergent configuration paradigms.

### **Custom Modes and Rule Enforcement in Agentic IDEs**

Within environments like Cursor and Windsurf, developers can define custom operational modes that fundamentally restrict the capabilities available to the model.40 A critical feature of these custom modes is the ability to explicitly toggle which overarching permissions the agent is granted. By defining a custom mode specifically tailored for the sandboxed workflow, developers can completely disable the IDE's native terminal execution permission while leaving protocol-based tool execution fully enabled.3 If the native terminal is disabled at the mode level, the model's localized tool schema will not contain the native execution endpoint. When the model searches its available functions, the custom protocol tool will be the only execution pathway mathematically available, enforcing absolute determinism without relying on complex, fragile prompt engineering.3  
Additionally, these environments heavily utilize markdown-based instruction files. While legacy systems relied on global rule files, modern implementations utilize sophisticated, glob-matched markdown files that apply specific instructions only when certain files are modified or specific conditions are met.2 To enforce sandbox usage, an instruction file must be generated with explicit frontmatter declaring alwaysApply: true. This directive is an architectural necessity, as it forces the environment to inject the sandbox rules into the context window for every single conversation turn, counteracting the natural decay of the model's attention mechanism.2

### **Silent Execution and Annotations in Visual Studio Code**

Visual Studio Code manages protocol servers either through dedicated extensions or direct workspace configurations.8 The platform's implementation includes unique metadata annotations that drastically alter how tools are presented to the end user.8 By applying specific annotations to the sandbox's non-destructive utilities—such as environment variable checks or log retrieval operations—the server can instruct Visual Studio Code to skip the standard user confirmation dialogs.8  
This autonomous execution capability minimizes workflow friction, allowing the model to chain multiple sandbox operations together at high speed without stalling the process to wait for human approval.8 When combined with dynamic tool discovery, Visual Studio Code can seamlessly transition the model between broad exploration and deep, autonomous debugging entirely within the secure confines of the sandbox.

### **Command-Line Interfaces and Configuration Overrides**

Command-line execution environments, such as Claude Code, present a different set of integration challenges, relying heavily on localized memory files and complex flag configurations.43 To ensure the sandboxed terminal is universally prioritized, setup scripts must utilize interface-specific flags to physically append custom instructions directly to the foundational system prompt.44 This architectural override forcefully deprecates default tool preferences without requiring the end-user to manually parse and manage complex YAML agent configuration files.44  
Furthermore, these command-line interfaces support the instantiation of isolated subagents. This allows for the creation of a dedicated sandbox agent with a strictly defined list of allowed tools, explicitly denying access to native shell execution capabilities at the initialization phase.43 By denying the tool before the session even begins, the model is forced to adapt to the provided sandbox tools, eliminating the possibility of unauthorized local execution.

### **Managing Token Constraints in Local Inference Platforms**

Platforms designed for localized inference, such as LM Studio, act as independent protocol hosts. They prioritize parsing tool requests directly into the standardized response objects utilized by the underlying inference engines.38 For these platforms, which may lack native terminal equivalents to compete with the sandbox, the primary architectural challenge is preventing catastrophic token overflow.38  
Users of these platforms frequently run heavily quantized models on constrained consumer hardware, resulting in extremely stringent context limits.38 The aforementioned progressive disclosure architecture is not merely an optimization in this context; it is an absolute necessity. If a server attempts to inject a massive tool schema into a constrained local model, it will immediately trigger a context overflow, rendering the entire system inoperable.38 The integration must detect the host environment and aggressively compress its tool schema, relying entirely on recursive discovery to navigate the available sandbox capabilities.

## **Zero-Friction Setup via the Plugin Ecosystem and Protocol Prompts**

A paramount requirement for widespread adoption is that the system must be effortless to configure, demanding zero complex setup from the end user while simultaneously minimizing reliance on manually maintained instructional files. The optimal deployment vehicle for achieving this seamless integration is the utilization of bundled IDE plugins and native protocol prompts.

### **Eliminating Configuration Overhead with Bundled Plugins**

Modern development environments have introduced plugin ecosystems that allow developers to package protocol servers, localized rules, execution skills, autonomous agents, and lifecycle hooks into a single, cohesive, one-click installation package.47 By bundling the sandbox integration into a structured plugin, the entire configuration hierarchy is automated.47  
When a user installs the plugin from the marketplace or via a command palette directive, the environment automatically scaffolds a hidden configuration directory containing the server registration parameters, the required lifecycle hooks for execution interception, and the necessary glob-matched rule files to guide the model's behavior.47 This architecture completely eliminates the need for manual JSON editing, environment variable configuration, or the maintenance of sprawling instructional documents.47 Upon installation, the execution interception hooks become immediately active, meaning that from the very first interaction, any terminal command the model attempts to execute is securely captured, audited, and routed to the sandboxed environment.33

### **Session Initialization via Protocol Prompts**

To fulfill the requirement of establishing an intuitive setup command—such as an initialization directive that requires no manual file creation—the integration must utilize the prompt primitives defined within the Model Context Protocol.8 Within this architecture, prompts are not static text files; they are reusable, parameterized templates dynamically exposed by the server itself.13  
When the server implements the required listing and retrieval operations for prompts, client applications natively render these as highly visible, interactive user interface elements, often presented as slash commands.12

1. The user simply types the initialization slash command into the application's chat interface.13  
2. The client application intercepts this command and requests the specific template from the connected server.13  
3. The server responds with a highly engineered set of system instructions, which are immediately injected into the active conversation context. This injection serves as an overriding directive, establishing the rigid boundaries of the execution sandbox for the duration of the session.13

Because this prompt template is fetched dynamically from the server at the moment of execution, it allows the integration developers to update the initialization logic, refine the semantic weighting, and adjust the behavioral rules globally. Users benefit from the most advanced routing instructions without ever needing to manually pull updates or edit localized rule files.50 It guarantees that the operational rules are mathematically fresh, contextually pristine, and immediately placed at the absolute forefront of the model's attention mechanism.

## **Security Architectures and Execution Boundaries**

Substituting the native terminal of a development environment with a protocol-driven sandbox introduces a profound paradigm shift in threat modeling and operational security. While sandboxing inherently improves the security posture by isolating the execution layer from the host machine, exposing highly capable execution tools to an autonomous intelligence creates complex vulnerabilities related to prompt injection, unauthorized privilege escalation, and uncontrolled resource consumption.18

### **Privilege Isolation and Transport-Layer Authorization**

To prevent unauthorized code execution—such as a model being manipulated by a maliciously crafted external dataset to execute destructive shell commands—the server must implement uncompromising privilege isolation architectures.52  
The protocol natively supports robust authentication methodologies through its transport layer. For standard input/output transports operating locally, execution is inherently bound to the permissions of the invoking user.8 However, for remote sandbox deployments or highly restricted multi-tenant environments, HTTP transports utilizing standardized OAuth scopes provide a highly granular, enterprise-grade defense mechanism.23 The sandbox server must implement comprehensive token verification logic to validate cryptographic access tokens prior to executing any incoming command payload.23 If the model attempts to execute a command that exceeds its authorized scope, or if a prompt injection attack attempts to subvert the sandbox parameters, the server must aggressively terminate the execution and return a protocol-level error. This forces the model to either halt its operation or request explicit privilege escalation directly from the human operator, ensuring that ultimate authority remains firmly in human hands.10

### **Defeating Inherent IDE Sandbox Bypasses**

In highly automated environments, users frequently grapple with a confusing array of built-in security features, such as external file protections and terminal command allowlists, which attempt to restrict the model's capabilities.55 However, exhaustive security analyses consistently indicate that these native protections are fundamentally flawed, routinely bypassed by models utilizing indirect terminal commands or obscure scripting pathways.56  
By enforcing an architecture where all execution is mandatorily routed through the custom protocol tool, security is entirely decoupled from the unreliable internal configurations of the host IDE.56 The standalone sandbox server assumes absolute, unyielding authority over the execution environment. Even if a user recklessly disables tool protection mechanisms within the IDE interface, the sandbox server continues to enforce its own internal, cryptographically secure allowlists.55 It maintains the ability to reject specific, highly destructive commands, block network exfiltration attempts, and monitor file modifications directly at the protocol layer, returning clear, instructional error messages to the model that guide it back toward secure operational patterns.55

## **Synthesized Strategic Roadmap for Deterministic Enforcement**

To achieve the deterministic, persistent, and entirely zero-setup integration of a custom sandboxed shell across the vastly complex modern artificial intelligence developer ecosystem, the following prioritized, highly technical roadmap must be adopted. These recommendations transcend theoretical prompt engineering, focusing entirely on structural protocol manipulation and client-level execution hijacking.

### **Priority 1: Mandatory Execution Interception via Lifecycle Hooks**

The foundational premise of this integration must be that Large Language Models are inherently unreliable and will inevitably attempt to utilize native execution pathways due to context degradation. It is imperative to cease relying on prompt engineering to prevent this behavior. Instead, the architecture must deploy intercept scripts tied directly to the host application's lifecycle hooks.33 By capturing the shell execution event before it is passed to the underlying operating system, the integration ensures that even when the model hallucinates or intentionally reverts to its default training paths, the command string is silently seized and forwarded to the designated sandbox. This is the only mathematically absolute method to overcome context decay and enforce strict behavioral boundaries.33

### **Priority 2: Semantic Weaponization and Protocol-Level Directives**

The tools exposed by the sandbox must be meticulously renamed to leverage maximal semantic weight, utilizing clear, intention-based identifiers that signal their critical nature to the model. Concurrently, the server must aggressively utilize the initialization response to inject non-negotiable, protocol-level instructions directly into the base configuration of the client.5 Embedding commanding keywords directly into the tool descriptions exploits the mathematical realities of gradient-based tool selection preferences, structurally biasing the model toward the sandbox long before it evaluates the conversation history.20

### **Priority 3: Aggressive Context Optimization via Progressive Disclosure**

To minimize the devastating impact of context bloat, the integration must abandon static tool registries. It must implement dynamic schema generation, utilizing protocol state change notifications to seamlessly alter the available toolset in real-time based on the immediate needs of the active workflow.24 The server must expose only a single, highly compressed discovery tool initially, expanding the schema to include granular execution and debugging capabilities only when the model explicitly requires them.27 This approach is an absolute requirement for ensuring compatibility with the constrained context windows of locally hosted models and preserving processing power for actual software engineering tasks.38

### **Priority 4: Frictionless Distribution Architectures**

The complexity of configuring protocol servers, defining lifecycle hooks, and establishing behavioral rules must be completely abstracted away from the end consumer. The entire integration package must be bundled into platform-specific extensions or utilized alongside natively exposed protocol prompts.47 By exposing initialization sequences as dynamic slash commands, users can instantly configure a highly secure, deterministic session with a single keystroke, entirely bypassing the overwhelming friction of managing sprawling markdown configurations and JSON manifests.12  
By deliberately intertwining aggressive semantic weighting with hard-coded lifecycle interceptions and highly optimized dynamic schema disclosure, this architecture allows custom execution environments to entirely transcend the probabilistic limitations of modern intelligence models. This multifaceted blueprint forces both the host application and the underlying model into a strictly controlled deterministic loop, ensuring that the custom sandbox is consistently prioritized, contextually invisible, and effortlessly integrated across the entire spectrum of major artificial intelligence development platforms.

#### **Works cited**

1. How to stop Cursor wanting to run terminal commands in chat window? \- Reddit, accessed February 28, 2026, [https://www.reddit.com/r/cursor/comments/1j1zewz/how\_to\_stop\_cursor\_wanting\_to\_run\_terminal/](https://www.reddit.com/r/cursor/comments/1j1zewz/how_to_stop_cursor_wanting_to_run_terminal/)  
2. Cursor often forgets .mdc instructions \- Bug Reports, accessed February 28, 2026, [https://forum.cursor.com/t/cursor-often-forgets-mdc-instructions/151718](https://forum.cursor.com/t/cursor-often-forgets-mdc-instructions/151718)  
3. How can I turn off trying to run commands in a terminal \- Cursor \- Community Forum, accessed February 28, 2026, [https://forum.cursor.com/t/how-can-i-turn-off-trying-to-run-commands-in-a-terminal/105586](https://forum.cursor.com/t/how-can-i-turn-off-trying-to-run-commands-in-a-terminal/105586)  
4. Skills are progressively disclosed, but MCP tools load all-at-once. How do we avoid context/tool overload with many MCP servers? : r/Anthropic \- Reddit, accessed February 28, 2026, [https://www.reddit.com/r/Anthropic/comments/1pumj33/skills\_are\_progressively\_disclosed\_but\_mcp\_tools/](https://www.reddit.com/r/Anthropic/comments/1pumj33/skills_are_progressively_disclosed_but_mcp_tools/)  
5. Lifecycle \- Model Context Protocol, accessed February 28, 2026, [https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle](https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle)  
6. Specification \- Model Context Protocol （MCP）, accessed February 28, 2026, [https://modelcontextprotocol.info/specification/2025-06-18/](https://modelcontextprotocol.info/specification/2025-06-18/)  
7. Connect to local MCP servers \- Model Context Protocol, accessed February 28, 2026, [https://modelcontextprotocol.io/docs/develop/connect-local-servers](https://modelcontextprotocol.io/docs/develop/connect-local-servers)  
8. MCP developer guide | Visual Studio Code Extension API, accessed February 28, 2026, [https://code.visualstudio.com/api/extension-guides/ai/mcp](https://code.visualstudio.com/api/extension-guides/ai/mcp)  
9. Build an MCP server \- Model Context Protocol, accessed February 28, 2026, [https://modelcontextprotocol.io/docs/develop/build-server](https://modelcontextprotocol.io/docs/develop/build-server)  
10. Tools \- Model Context Protocol, accessed February 28, 2026, [https://modelcontextprotocol.io/legacy/concepts/tools](https://modelcontextprotocol.io/legacy/concepts/tools)  
11. Tools \- Model Context Protocol, accessed February 28, 2026, [https://modelcontextprotocol.io/specification/2025-06-18/server/tools](https://modelcontextprotocol.io/specification/2025-06-18/server/tools)  
12. Add and manage MCP servers in VS Code, accessed February 28, 2026, [https://code.visualstudio.com/docs/copilot/customization/mcp-servers](https://code.visualstudio.com/docs/copilot/customization/mcp-servers)  
13. Understanding MCP servers \- Model Context Protocol, accessed February 28, 2026, [https://modelcontextprotocol.io/docs/learn/server-concepts](https://modelcontextprotocol.io/docs/learn/server-concepts)  
14. When the Agent mode executes a command in the Terminal, it often stops automatically\!, accessed February 28, 2026, [https://forum.cursor.com/t/when-the-agent-mode-executes-a-command-in-the-terminal-it-often-stops-automatically/144230](https://forum.cursor.com/t/when-the-agent-mode-executes-a-command-in-the-terminal-it-often-stops-automatically/144230)  
15. MCP Server | IntelliJ IDEA Documentation \- JetBrains, accessed February 28, 2026, [https://www.jetbrains.com/help/idea/mcp-server.html](https://www.jetbrains.com/help/idea/mcp-server.html)  
16. How I structure Claude Code projects (CLAUDE.md, Skills, MCP) : r/ClaudeAI \- Reddit, accessed February 28, 2026, [https://www.reddit.com/r/ClaudeAI/comments/1r66oo0/how\_i\_structure\_claude\_code\_projects\_claudemd/](https://www.reddit.com/r/ClaudeAI/comments/1r66oo0/how_i_structure_claude_code_projects_claudemd/)  
17. MCP API Command Method Naming Best Practices \- GitHub Gist, accessed February 28, 2026, [https://gist.github.com/eonist/eb8d5628aad07fc57ce339e518158c20](https://gist.github.com/eonist/eb8d5628aad07fc57ce339e518158c20)  
18. MCP Security Exposed: What You Need to Know Now | Palo Alto Networks, accessed February 28, 2026, [https://live.paloaltonetworks.com/t5/community-blogs/mcp-security-exposed-what-you-need-to-know-now/ba-p/1227143](https://live.paloaltonetworks.com/t5/community-blogs/mcp-security-exposed-what-you-need-to-know-now/ba-p/1227143)  
19. Deep Dive MCP and A2A Attack Vectors for AI Agents \- Solo.io, accessed February 28, 2026, [https://www.solo.io/blog/deep-dive-mcp-and-a2a-attack-vectors-for-ai-agents](https://www.solo.io/blog/deep-dive-mcp-and-a2a-attack-vectors-for-ai-agents)  
20. MPMA: Preference Manipulation Attack Against Model Context Protocol \- arXiv, accessed February 28, 2026, [https://arxiv.org/html/2505.11154v2](https://arxiv.org/html/2505.11154v2)  
21. A Survey of the Model Context Protocol (MCP): Standardizing Context to Enhance Large Language Models (LLMs) \- Preprints.org, accessed February 28, 2026, [https://www.preprints.org/manuscript/202504.0245](https://www.preprints.org/manuscript/202504.0245)  
22. Schema Reference \- Model Context Protocol, accessed February 28, 2026, [https://modelcontextprotocol.io/specification/draft/schema](https://modelcontextprotocol.io/specification/draft/schema)  
23. modelcontextprotocol/python-sdk: The official Python SDK for Model Context Protocol servers and clients \- GitHub, accessed February 28, 2026, [https://github.com/modelcontextprotocol/python-sdk](https://github.com/modelcontextprotocol/python-sdk)  
24. Tools \- Model Context Protocol, accessed February 28, 2026, [https://modelcontextprotocol.io/docs/concepts/tools](https://modelcontextprotocol.io/docs/concepts/tools)  
25. Moving Claude Skills Server-Side with Remix Servers \- Prefect, accessed February 28, 2026, [https://www.prefect.io/blog/moving-claude-skills-server-side-with-remix-servers](https://www.prefect.io/blog/moving-claude-skills-server-side-with-remix-servers)  
26. Overriding MCP tool name, description, and input schema using a proxy \- Reddit, accessed February 28, 2026, [https://www.reddit.com/r/mcp/comments/1nh2c76/overriding\_mcp\_tool\_name\_description\_and\_input/](https://www.reddit.com/r/mcp/comments/1nh2c76/overriding_mcp_tool_name_description_and_input/)  
27. Progressive Disclosure for Typed Library Discovery & Introspection in MCP \#631 \- GitHub, accessed February 28, 2026, [https://github.com/orgs/modelcontextprotocol/discussions/631](https://github.com/orgs/modelcontextprotocol/discussions/631)  
28. Lessons Learned Writing an MCP Server for PostgreSQL \- pgEdge, accessed February 28, 2026, [https://www.pgedge.com/blog/lessons-learned-writing-an-mcp-server-for-postgresql](https://www.pgedge.com/blog/lessons-learned-writing-an-mcp-server-for-postgresql)  
29. Code execution with MCP: building more efficient AI agents \- Anthropic, accessed February 28, 2026, [https://www.anthropic.com/engineering/code-execution-with-mcp](https://www.anthropic.com/engineering/code-execution-with-mcp)  
30. Add support for MCP dynamic tool update by \`notifications/tools/list\_changed\` · Issue \#13850 · google-gemini/gemini-cli \- GitHub, accessed February 28, 2026, [https://github.com/google-gemini/gemini-cli/issues/13850](https://github.com/google-gemini/gemini-cli/issues/13850)  
31. Dramatically Reducing AI Agent Token Usage with MCP Code Execution \- Medium, accessed February 28, 2026, [https://medium.com/@shamsul.arefin/building-an-ai-agent-with-mcp-code-execution-from-confusion-to-clarity-6b13fccc8c4b](https://medium.com/@shamsul.arefin/building-an-ai-agent-with-mcp-code-execution-from-confusion-to-clarity-6b13fccc8c4b)  
32. Is there a way to configure Cursor so that it can access or interact with the terminal directly?, accessed February 28, 2026, [https://forum.cursor.com/t/is-there-a-way-to-configure-cursor-so-that-it-can-access-or-interact-with-the-terminal-directly/14116](https://forum.cursor.com/t/is-there-a-way-to-configure-cursor-so-that-it-can-access-or-interact-with-the-terminal-directly/14116)  
33. Hooks | Cursor Docs, accessed February 28, 2026, [https://cursor.com/docs/agent/hooks](https://cursor.com/docs/agent/hooks)  
34. meta-hook-create | Skills Marketplace \- LobeHub, accessed February 28, 2026, [https://lobehub.com/de/skills/jurooravec-agents-hook-create](https://lobehub.com/de/skills/jurooravec-agents-hook-create)  
35. Third Party Hooks | Cursor Docs, accessed February 28, 2026, [https://cursor.com/docs/agent/third-party-hooks](https://cursor.com/docs/agent/third-party-hooks)  
36. Inconsistent working directory for plugin hook commands \- Cursor \- Community Forum, accessed February 28, 2026, [https://forum.cursor.com/t/inconsistent-working-directory-for-plugin-hook-commands/153236](https://forum.cursor.com/t/inconsistent-working-directory-for-plugin-hook-commands/153236)  
37. How to use MCP in cursor ? Step-By-Step Tutorial \- YouTube, accessed February 28, 2026, [https://www.youtube.com/watch?v=J\_OSlWqaVo0](https://www.youtube.com/watch?v=J_OSlWqaVo0)  
38. Use MCP Servers | LM Studio Docs, accessed February 28, 2026, [https://lmstudio.ai/docs/app/mcp](https://lmstudio.ai/docs/app/mcp)  
39. How to Add MCP Servers to Google's Antigravity IDE, accessed February 28, 2026, [https://www.youtube.com/watch?v=TwRPGmBKIY0](https://www.youtube.com/watch?v=TwRPGmBKIY0)  
40. Return the Custom Modes features \- Cursor \- Community Forum, accessed February 28, 2026, [https://forum.cursor.com/t/return-the-custom-modes-features/144170](https://forum.cursor.com/t/return-the-custom-modes-features/144170)  
41. Supercharged Test Running with Custom Modes in Cursor IDE | Bright Inventions, accessed February 28, 2026, [https://brightinventions.pl/blog/supercharged-test-running-with-custom-modes-in-cursor-ide/](https://brightinventions.pl/blog/supercharged-test-running-with-custom-modes-in-cursor-ide/)  
42. Optimal structure for .mdc rules files \- Discussions \- Cursor \- Community Forum, accessed February 28, 2026, [https://forum.cursor.com/t/optimal-structure-for-mdc-rules-files/52260](https://forum.cursor.com/t/optimal-structure-for-mdc-rules-files/52260)  
43. Claude Code settings \- Claude Code Docs, accessed February 28, 2026, [https://code.claude.com/docs/en/settings](https://code.claude.com/docs/en/settings)  
44. CLI reference \- Claude Code Docs, accessed February 28, 2026, [https://code.claude.com/docs/en/cli-reference](https://code.claude.com/docs/en/cli-reference)  
45. " \--append-system-prompt  
46. Tool Use | LM Studio Docs, accessed February 28, 2026, [https://lmstudio.ai/docs/developer/openai-compat/tools](https://lmstudio.ai/docs/developer/openai-compat/tools)  
47. Building Plugins | Cursor Docs, accessed February 28, 2026, [https://cursor.com/docs/plugins/building](https://cursor.com/docs/plugins/building)  
48. Cursor 2.5: Plugins \- Release Discussions, accessed February 28, 2026, [https://forum.cursor.com/t/cursor-2-5-plugins/152124](https://forum.cursor.com/t/cursor-2-5-plugins/152124)  
49. The Complete MCP Experience: Full Specification Support in VS Code, accessed February 28, 2026, [https://code.visualstudio.com/blogs/2025/06/12/full-mcp-spec-support](https://code.visualstudio.com/blogs/2025/06/12/full-mcp-spec-support)  
50. Expose MCP Server Prompts as Slash Commands Like Claude Code · Issue \#8342 \- GitHub, accessed February 28, 2026, [https://github.com/openai/codex/issues/8342](https://github.com/openai/codex/issues/8342)  
51. Computer use tool \- Claude API Docs, accessed February 28, 2026, [https://platform.claude.com/docs/en/agents-and-tools/tool-use/computer-use-tool](https://platform.claude.com/docs/en/agents-and-tools/tool-use/computer-use-tool)  
52. Building Own MCP \- Augmented LLM for Threat Hunting \- Tier Zero Security, accessed February 28, 2026, [https://tierzerosecurity.co.nz/2025/04/29/mcp-llm.html](https://tierzerosecurity.co.nz/2025/04/29/mcp-llm.html)  
53. Model Context Protocol \- OpenAI for developers, accessed February 28, 2026, [https://developers.openai.com/codex/mcp/](https://developers.openai.com/codex/mcp/)  
54. What Is the Model Context Protocol (MCP) and How It Works \- Descope, accessed February 28, 2026, [https://www.descope.com/learn/post/mcp](https://www.descope.com/learn/post/mcp)  
55. Terminal | Cursor Docs, accessed February 28, 2026, [https://cursor.com/docs/agent/terminal](https://cursor.com/docs/agent/terminal)  
56. Cursor IDE Security Best Practices & Tips \- Backslash, accessed February 28, 2026, [https://www.backslash.security/blog/cursor-ide-security-best-practices](https://www.backslash.security/blog/cursor-ide-security-best-practices)